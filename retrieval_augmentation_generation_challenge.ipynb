{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974ac24-0b81-43eb-a8b3-5bc86d402552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip3 install -q google-cloud-aiplatform\n",
    "!pip3 install -q langchain-google-vertexai\n",
    "!pip3 install -q langchain-google-genai\n",
    "!pip3 install -q wikipedia\n",
    "!pip3 install -q chromadb==0.5.3\n",
    "!pip3 install -q langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92d0b4-d250-4a29-ae4b-5f296f14e896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart the kernel after libraries are loaded\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68059ac8-7ae1-4e87-8460-55ec0796ba01",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607b448-c577-49b6-bf19-41856955b91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b54483-cba4-4191-a3d3-841113acc3fa",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672edc04-3d94-4d8c-8eda-05f0577dd70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain._api import LangChainDeprecationWarning\n",
    "warnings.simplefilter(\"ignore\", category=LangChainDeprecationWarning)\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da463ab-5ee2-43cb-a9f0-0c6b0fca00a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "PROJECT_ID = subprocess.check_output([\"gcloud\", \"config\", \"get-value\", \"project\"], text=True).strip()\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef538a11-c2bd-467d-84d8-453b7243dff7",
   "metadata": {},
   "source": [
    "## Task 1. Load `Documents` from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8eb2ff-bfee-488e-8d38-f7cf68ea9b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the LangChain documentation to load documents for the query below\n",
    "# Set the following parameters:\n",
    "#  * query: \"Gemini GPT-4\"\n",
    "#  * load_max_docs: 10\n",
    "# https://python.langchain.com/docs/integrations/document_loaders/wikipedia\n",
    "\n",
    "query=\"Gemini GPT-4\"\n",
    "max_docs=10\n",
    "\n",
    "documents = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608f4d6-4572-4600-906e-b7aed7f9e5e4",
   "metadata": {},
   "source": [
    "## Task 2. Use `RecursiveTextSplitter` to split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc5f5f-0274-4239-b9b0-7c1855011426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the LangChain documentation to split the docs loaded into smaller chunks for indexing\n",
    "# https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"# of documents = {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262239a-a798-4417-aaa2-7c1536d7c1e2",
   "metadata": {},
   "source": [
    "## Task 3. Index Documents in Chroma DB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09a444-42c2-42fa-9c85-73bde82a346a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert the correct model name in the constructor below\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#models\n",
    "# You can ignore warning messages when running this cell\n",
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef3f05-83eb-4fb3-b864-aec44e6dd257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference the correct parameters (already defined) to properly index \n",
    "# the documents loaded from Wikipedia into Chroma DB as embeddings\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,                 # Data\n",
    "    embedding=embeddings,           # Embedding model\n",
    "    persist_directory=\"./chroma_db\" # Directory to save data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee92709-22b6-435c-930f-c33dc309b700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore_disk = Chroma(\n",
    "    persist_directory=\"./chroma_db\", # Directory of db\n",
    "    embedding_function=embeddings    # Embedding model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa643da-2f16-4914-a150-a6045c55d866",
   "metadata": {},
   "source": [
    "## Task 4. Setup a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40965d2b-1858-4189-a553-2baab720d420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Chroma DB as a `Retriever` for querying the documents\n",
    "# set the k value to 10\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/chroma#retriever-options\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef6a2d-68a6-41d6-8005-e3fbfd48c1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the retriever with a query\n",
    "doc = retriever.invoke(\"Google Gemini\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486a26e-842a-444d-b81b-95752d3effce",
   "metadata": {},
   "source": [
    "## Task 5. Setup Model and Build LangChain `Chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a0e90-3c0f-4a2b-bb90-c147b5d8b1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert the correct model name in the constructor below.\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models\n",
    "# Ensure that the output is the least random configurable\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\", temperature=0.9, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464dc6a-5b4c-49f2-88ae-7e0eb014d1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4676768-3023-498e-a21c-1f9a66aaa898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c891317-71ea-4f71-9aaa-64134472e494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete the Chain in the correct order. You need to leverage the `prompt` and `model` defined\n",
    "# in earlier cells in the correct order to run the next cell successfully by replacing CHAIN_1 and CHAIN_2.\n",
    "chain = (\n",
    "    { \"context\": retriever | format_docs, \"question\": RunnablePassthrough() }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2173f89-1417-4259-a9d6-eeb85e739fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke(\"What is Gemini?\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-16.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-16:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
